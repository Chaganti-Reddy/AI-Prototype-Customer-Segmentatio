#+title: *Customer Segmentation Analysis Model of E-Commerce Industry*
#+SUBTITLE: *AI Product Prototype Analysis*
#+AUTHOR: /Chaganti Venkatarami Reddy, Subesha Sasmal, Parth Sukla/
#+OPTIONS: date:nil toc:nil num:nil
#+LATEX_HEADER: \usepackage{draftwatermark}
#+SETUPFILE: /home/reddy/Documents/GitHub/dotfiles/org/latex-standard-reddy.org
#+LATEX_CLASS_OPTIONS: [a4paper, 11pt]

#+BEGIN_EXPORT latex
\SetWatermarkLightness{ 0.94 }
\SetWatermarkText{\textsc{Feynn Labs}}
% \SetWatermarkColor[gray]{0.5}
% \SetWatermarkColor[rgb]{1,0,0}
\SetWatermarkScale{ 0.67 }
% \SetWatermarkFontSize{2cm}

\let\oldquote\quote
\let\endoldquote\endquote
\renewenvironment{quote}[2][]
  {\if\relax\detokenize{#1}\relax
     \def\quoteauthor{#2}%
   \else
     \def\quoteauthor{#2~---~#1}%
   \fi
   \oldquote}
  {\par\nobreak\smallskip\hfill(\quoteauthor)%
   \endoldquote\addvspace{\bigskipamount}}

#+END_EXPORT

#+BEGIN_EXPORT latex
%\vspace{3cm}
\begin{center}
\begin{figure}[h!]
\hspace{6mm} \includegraphics[width=400px, height=280px]{./images/1.png}
\vspace{2mm}
\caption*{\hspace{12mm} AI Prototype Analysis for Customer Segmentation}
\end{figure}
\end{center}
\vspace{10mm}
\hline
\vspace{10mm}
\begin{quote}{Philip Kotler}
\bsifamily {To be useful, segments must be measurable, substantial, accessible, differentiable, and accountable.}
\end{quote}

\clearpage
#+END_EXPORT

@@latex:\clearpage@@

* *Abstract*

Artificial intelligence (AI) has the potential to revolutionize pathology. AI refers to the application of modern machine learning techniques to digital tissue images in order to detect, quantify, or characterize specific cell or tissue structures. By automating time‑consuming diagnostic tasks, AI can greatly reduce the workload and help to remedy the serious shortage of pathologists. At the same time, AI can make analyses more sensitive and reproducible and it can capture novel biomarkers from tissue morphology for precision medicine. In a survey from the year 2019 of 487 pathologists from 54 countries, a great majority looked forward to using AI as a diagnostic tool.

In this report we are going to analyse and explore a E-Commerce bussiness data. And we will create a model to make customer segments based on new customers and their sales.

@@latex:\vspace{5mm}@@

*KeyWords* : /Customers, Segments, E-Commerce, Prototypes, Market segmentation, Stock Code, Basket Price, Cluster analysis, Regression./

@@latex:\vspace{8mm}@@

* *Data Collection*

The data has been collected manually, and the sources used for this process are listed below :

 + https://www.kaggle.com/datasets
 + https://data.gov.in/
 + https://www.data.gov/
 + https://data.worldbank.org/
 + https://datasetsearch.research.google.com/

@@latex:\vspace{1cm}@@

@@latex:\clearpage@@

* *Business Need Assessment*

   + Many E-Commerce businessess should have a mandatory analysis of their previous sales and they have to create new segments based on new customers.
   + Analysing their datasets and forecatsing their sales will enable them to buy grocery, according, prepare the types of items that their customers like during a particular time of the year.
   + There are lot of E-Commerce bussinessess of same type in the city, so this is a big market but we have to *Segment the Market* according to which item is mostly selling.
   + Properly done, the model can be extended to whole E-Commerce businessess.


   [[./images/CS.png]]

@@latex:\vspace{10mm}@@

* *Target Specifications and Characterizations*

 + The target here is to develop a model that will forecast future sales, cut down costs and new segments(customers) of an E-Commerce Industry.
 + The trend recognition has to be done by a data scientist who has some knowledge about the E-Commerce industry.
 + Employing someone who has no knowledge about the E-Commerce industry is not a great idea as they will not be able to give the insights of someone who knows about this field.
 + The model should be able to handle large volumes of data, as in a E-Commerce industry there will be a lot of features for the model to look at and the size of data depends on the sales of a particular month or week.
 + We should also know in advance whether the customers need our model to forecast the sales for a week or for a month.


* Benchmarking Alternative Products

 Many big companies like *Amazon, Flipkart*  have started implementing ML/AI in their outlets and while they are making new products as well. These companies have identified the use of ML and AI and are benefitting from it. A lot of AI companies have also entered the market helping big corporations.

* Applicable Patents

[[https://patents.google.com/patent/US10990644B2/en?q=customer+segmentation&oq=customer+segmentation][Systems and methods for contextual vocabularies and customer segmentation]] by /SDL Netherlands B.V. , Amsterdam Zuidoost ( NL )/ was patented on 2011-01-29.

* Applicable Regulations:

 + Data Protection and Privacy Rules.
 + License for the open-source codes that might be used in the model implementation.
 + Laws related to AI.

* Applicable Constraints

 + Data Collection from the customer.
 + The customer should know about the time, money and scope of the project before it starts.
 + Transperant use of the data obtained from the customer.

@@latex:\clearpage@@

* Bussiness Opportunities

 The target customers here are mainly local E-Commerce Industries or stores who have a good number of customers.

 Many local E-Commerce stores might be stuck at the same level for some period of time without knowing what to do, to generate more revenue. The main goal while implementing an ML model for their bussiness will be to reduce their cost by suggesting what sort of materials the customers are more buying at a particular time of the year. They are the primary bussiness targets.

 If the customer has a delivery option like *Amazon, Flipkart* etc.,, then we will be able to find the areas that requires more deliveries and weed out areas that are not bringing much profit.

If the model is successfully implemented for the previously mentioned targets,
the model can be expanded for E-Commerce Industries that are in multiple.

* Concept Development

 We must first understand the environment before we start working on a model and the type of items, the people in that region like and what are the traditions there. After gaining sufficient knowledge about the environment we have to start collecting data. After collecting the data, we have to perform EDA which is used to identify patterns in the dataset and it will help us zone in on the areas that are leaking money. Visualization will help a lot here. Once we have found the trend and outliers, the next step is to use the basic regression models and time-series models , in which we will fit our training dataset and see what sort of results we will be getting. After analysing various parameters like squared-error, etc we will know what type of model to use and what type of model should our model be based around. The models will be regression models and time-series models.

* Implementation

** Packages/Tools Used:

1. *Numpy:* To calculate various calculations related to arrays .
2. *Pandas:* To read or load the datasets.
3. *SKLearn:* This is a Machine Learning library which contains builtin Machine Learning algorithms.

   @@latex:\clearpage@@

** Data Preprocessing

*Data Cleaning*

 The data collected is compact and is partly used for visualization purposes and partly for clustering. Python libraries such as NumPy, Pandas, Scikit-Learn, and SciPy are used for the workflow, and the results obtained are ensured to be reproducible.

#+ATTR_LATEX: :scale 0.7
    [[./images/2.png]]


#+ATTR_LATEX: :height 230px :width 400px
    [[./images/3.png]]

** EDA

We start the Exploratory Data Analysis with some data Analysis drawn from the data without Principal Component Analysis and with some Principal Component Analysis in the dataset obtained from the combination of all the data we have. PCA is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. These new transformed features are called the Principal Components. The process helps in reducing dimensions of the data to make the process of classification/regression or any form of machine learning, cost-effective.

*Exploring the content of variables*

This dataframe contains 8 variables that correspond to:

+ *InvoiceNo:* Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.
+ *StockCode:* Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.
+ *Description:* Product (item) name. Nominal.
+ *Quantity:* The quantities of each product (item) per transaction. Numeric.
+ *InvoiceDate:* Invoice Date and time. Numeric, the day and time when each transaction was generated.
+ *UnitPrice:* Unit price. Numeric, Product price per unit in sterling.
+ *CustomerID:* Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.
+ *Country:* Country name. Nominal, the name of the country where each customer resides.

Let's have a look at no.of orders from different countries:

#+ATTR_LATEX: :scale 0.78
[[./images/4.png]]

We see that the dataset is largely dominated by orders made from the UK.

@@latex:\clearpage@@

*Customers and Products*

It can be seen that the data concern 4372 users and that they bought 3684 different products. The total number of transactions carried out is of the order of  ∼ 22'000.

Now we will determine the number of products purchased in every transaction:

#+ATTR_LATEX: :height 290px :width 470px
    [[./images/5.png]]

The first lines of this list shows several things worthy of interest:

+ The existence of entries with the prefix C for the InvoiceNo variable: this indicates transactions that have been canceled.
+ The existence of users who only came once and only purchased one product (e.g. nº12346).
+ The existence of frequent users that buy a large number of items at each order.

@@latex:\clearpage@@

*Cancelling Orders*

First of all, we count the number of transactions corresponding to canceled orders:

@@latex:\vspace{10mm}@@

#+ATTR_LATEX: :width 470px :height 280px
[[./images/6.png]]

We note that the number of cancellations is quite large ( ∼ 16% of the total number of transactions).

On these few lines, we see that when an order is canceled, we have another transactions in the dataframe, mostly identical except for the *Quantity* and *InvoiceDate* variables. We decide to check if this is true for all the entries. To do this, I decide to locate the entries that indicate a negative quantity and check if there is systematically an order indicating the same quantity (but positive), with the same description (*CustomerID*, *Description* and *UnitPrice*):

#+ATTR_LATEX: :height 200px :width 470px
[[./images/7.png]]

*Breakdown of Order Amounts*

#+ATTR_LATEX: :height 280px :width 450px
[[./images/8.png]]

 It can be seen that the vast majority of orders concern relatively large purchases given that  ∼ 65% of purchases give prizes in excess of £ 200.

@@latex:\clearpage@@

 *Most Common Keywords*

#+ATTR_LATEX: :height 600px :width 470px
[[./images/9.png]]

@@latex:\clearpage@@

*Creating Clusters of products*

In the case of matrices with binary encoding, the most suitable metric for the calculation of distances is the Hamming's metric. In order to define (approximately) the number of clusters that best represents the data, we use the silhouette score:

#+ATTR_LATEX: :height 200px :width 470px
    [[./images/10.png]]

In practice, the scores obtained above can be considered equivalent since, depending on the run, scores of  0.1±0.05  will be obtained for all clusters with n_clusters  >  3 (we obtain slightly lower scores for the first cluster). On the other hand, we found that beyond 5 clusters, some clusters contained very few elements. we therefore choose to separate the dataset into 5 clusters. In order to ensure a good classification , we iterate until we obtain the best possible silhouette score, which is, in the present case, around 0.15:

#+ATTR_LATEX: :height 230px :width 470px
[[./images/11.png]]

@@latex:\clearpage@@

** Characterizing the content of clusters

+ *Silhouette intra-cluster score*

  In order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters.

#+ATTR_LATEX: :height 480px :width 470px
  [[./images/12.png]]

@@latex:\clearpage@@

+ *Word Cloud*

  Now we can have a look at the type of objects that each cluster represents. In order to obtain a global view of their contents, we determine which keywords are the most frequent in each of them and I output the result as wordclouds:

@@latex:\vspace{5mm}@@

#+ATTR_LATEX: :height 450px :width 470px
    [[./images/13.png]]

@@latex:\vspace{10mm}@@

From this representation, we can see that for example, one of the clusters contains objects that could be associated with gifts (keywords: Christmas, packaging, card, ...). Another cluster would rather contain luxury items and jewelry (keywords: necklace, bracelet, lace, silver, ...). Nevertheless, it can also be observed that many words appear in various clusters and it is therefore difficult to clearly distinguish them.

+ *Principal Component Analysis*

  In order to ensure that these clusters are truly distinct, we look at their composition. Given the large number of variables of the initial matrix, we first perform a PCA, and then check for the amount of variance explained by each component:

@@latex:\vspace{10mm}@@

 #+ATTR_LATEX: :height 400px :width 480px
    [[./images/14.png]]

 @@latex:\clearpage@@

*Visualizing the Decomposed data*

 #+ATTR_LATEX: :height 530px :width 480px
    [[./images/15.png]]

 @@latex:\clearpage@@

 *Data Encoding*

 #+ATTR_LATEX: :height 210px :width 480px
 [[./images/16.png]]

*Creation of Customer Categories*

+ *Report via PCA*

 #+ATTR_LATEX: :height 330px :width 450px
  [[./images/17.png]]

@@latex:\clearpage@@

+ *Score of silhouette intra-cluster*

  #+ATTR_LATEX: :height 530px :width 430px
  [[./images/18.png]]

  @@latex:\clearpage@@

+ *Customers morphology*

  #+ATTR_LATEX: :height 560px :width 450px
 [[./images/19.png]]

  @@latex:\clearpage@@

* Classification of Customers

** Support Vector Machine Classifier (SVC)

The first classifier we use is the SVC classifier. In order to use it, we create an instance of the Class_Fit class and then callgrid_search(). When calling this method, we provide as parameters:

 + The hyperparameters for which I will seek an optimal value
 + The number of folds to be used for cross-validation

    #+ATTR_LATEX: :height 560px :width 450px
 [[./images/20.png]]

*** Confusion Matrix

  #+ATTR_LATEX: :height 300px :width 420px
 [[./images/21.png]]

@@latex:\clearpage@@

*** SVC Learning Curve

A typical way to test the quality of a fit is to draw a learning curve. In particular, this type of curves allow to detect possible drawbacks in the model, linked for example to over- or under-fitting. This also shows to which extent the mode could benefit from a larger data sample.

  #+ATTR_LATEX: :height 300px :width 420px
 [[./images/22.png]]

** Logistic Regression

*Percentage of Accuracy*

  #+ATTR_LATEX: :height 170px :width 420px
 [[./images/23.png]]

*** Logistic Regression Learning Curve

    #+ATTR_LATEX: :height 340px :width 420px
 [[./images/24.png]]


* K-Nearest Neighbors

*Percentage of Accuracy*

  #+ATTR_LATEX: :height 170px :width 420px
 [[./images/25.png]]

*** Nearest Neighbors Learning Curve

    #+ATTR_LATEX: :height 340px :width 420px
 [[./images/26.png]]

* Decision Tree

*Percentage of Accuracy*

  #+ATTR_LATEX: :height 100px :width 400px
 [[./images/27.png]]

*** Decision Tree Learning Curve

    #+ATTR_LATEX: :height 340px :width 420px
 [[./images/28.png]]

* Random Forest

*Percentage of Accuracy*

  #+ATTR_LATEX: :height 100px :width 400px
 [[./images/29.png]]

*** Random Forest Learning Curve

    #+ATTR_LATEX: :height 340px :width 420px
 [[./images/30.png]]

* AdaBoost Classifier

*Percentage of Accuracy*

  #+ATTR_LATEX: :height 100px :width 400px
 [[./images/31.png]]

*** AdaBoost Classifier Learning Curve

    #+ATTR_LATEX: :height 340px :width 420px
 [[./images/32.png]]

* Gradient Boosting Classifier

*Percentage of Accuracy*

  #+ATTR_LATEX: :height 100px :width 400px
 [[./images/33.png]]

*** Gradient Boosting Classifier Learning Curve

    #+ATTR_LATEX: :height 340px :width 420px
 [[./images/34.png]]


For all these, the classifier is based on 5 variables which are:

 + *Mean:* amount of the basket of the current purchase
 + *Categ_N:* with  N∈[0:4] : percentage spent in product category with index  N

Finally, the quality of the predictions of the different classifiers was tested over the last two months of the dataset. The data were then processed in two steps: first, all the data was considered (over the 2 months) to define the category to which each client belongs, and then, the classifier predictions were compared with this category assignment. I then found that 75% of clients are awarded the right classes. The performance of the classifier therefore seems correct given the potential shortcomings of the current model. In particular, a bias that has not been dealt with concerns the seasonality of purchases and the fact that purchasing habits will potentially depend on the time of year (for example, Christmas ). In practice, this seasonal effect may cause the categories defined over a 10-month period to be quite different from those extrapolated from the last two months. In order to correct such bias, it would be beneficial to have data that would cover a longer period of time.

* Final Product Prototype Details

The final product provides service to operators about the most bought combinations of products for them to analyze customer shopping patterns and helps them manage their inventory and also create new strategies and schemes to increase their sales.The service implements the Customer Segmentation Analysis, i.e Association Rule Mining technique on the dataset of transactions collected from E-Commerce Industries.

Some dynamics of the Apriori Algorithm used in this model and their meaning.

1. *Support:* It tells us about the combination of items bought together frequently. It gives the part of transactions that contain both A and B.

    #+begin_export latex
    $$Support = \frac{freq(A,B)}{N}$$
    #+end_export

2. *Confidence:* It tells us how frequently the items A and B are bought together, for the no. of  times A is bought.

     #+begin_export latex
    $$Confidence = \frac{freq(A,B)}{freq(A)}$$
    #+end_export

3. *Lift:* It indicates the strength of a rule over the randomness of A and B being bought together. It basically measures the strength of any association rule.

  #+begin_export latex
    $$Lift = \frac{Support)}{Supp(A) x Supp(B)}$$
    #+end_export

** 1. Feasibility

This project can be developed and deployed within a few years as SaaS( Software as a Service) for anyone to use.

** 2. Viability

As the retail industry grows in India and the world, there will always be small businesses existing which can use this service to improvise on their sales and data warehousing techniques. So, it is viable to survive in the long-term future as well but improvements are necessary as new technologies emerge.

** 3. Monetization

This service is directly monetizable as it can be directly released as a service on completion which can be used by businesses.

* Business Modeling

For this service, it is beneficial to use a Subscription Based Model, where initially some features will be provided for free to engage customer retention and increase our customer count. Later it will be charged a subscription fee to use the service further for their business.In the subscription business model, customers pay a fixed amount of money on fixed time intervals to get access to the product or service provided by the company. The major problem is user conversion; how to convert the users into paid users.

 #+ATTR_LATEX: :height 250px :width 420px
    [[./images/sub.png]]

* Financial Modeling

It can be directly launched into the retail market.

Let's consider our price of product = 250 for getting our graph

#+ATTR_LATEX: :height 250px :width 430px
 [[./images/graph.png]]

Financial Equation

#+begin_export latex
{\Large{$$Y = X^*(1+r)^t$$}}

{\Large{$$Y = (X)^*(3.2)^t$$}}

Y = Profit over Time

X = Price of our Product

r = Growth Rate

t = Time Interval

$\textbf{1+r = 1 + 3.2\% = 1.032}$
#+end_export

* Conclusion

Customer Segmentation analysis is being used by an increasing number of companies to acquire beneficial insights about associations and hidden relationships. However, for small businesses, this extension is a fantastic opportunity to boost sales and help them develop and grow their business.

@@latex:\clearpage@@

#+begin_export latex
    \begin{thebibliography}{9}
    \bibitem{texbook}
     Homeyer A, Lotz J, Schwen LO, Weiss N, Romberg D, Höfener H, et al.\emph{Artificial intelligence in pathology: From prototype to product}, J Pathol Inform 2021;12:13.

    \bibitem{MSA}
    Dolnicar, S., Grün Bettina, &amp; Leisch, F. (2019). \emph{Market segmentation analysis understanding it, doing it and making it useful}. Springer Nature.

    \bibitem{MS}
    McDonald, M., &amp; Dunbar, I. (2003). \emph{Market segmentation}. Butterworth-Heinemann.

    \bibitem{CS}
    Qualtrics AU. 2022. \emph{Customer Segmentation: Definition & Methods}. [online] Available at: \href{https://www.qualtrics.com/au/experience-management/brand/customer-segmentation/?rid=ip&prevsite=en&newsite=au&geo=IN&geomatch=au}{Customer Segmentation by Qualtrics Experience Management}.
    \end{thebibliography}

        \vspace{40mm}
      {\hspace{2.5mm} \bsifamily \huge{ Github :  \href{https://github.com/Chaganti-Reddy/AI-Prototype-Customer-Segmentation}{Chaganti Reddy/AI-Prototype}}}

        #+end_export
